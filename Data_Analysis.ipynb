{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Car Accident Severity__\n",
    "\n",
    "The objective of the project is to develop an automatic learning model / solution that allows identifying the main factors that affect the severity of an accident. For this, a database is provided that consists of information on the weather, the direction of the vehicles, the type of light that existed in the environment, the number of people involved in the accident, among others.\n",
    "\n",
    "The Target variable is severity, which has the following codes related to metadata:\n",
    "\n",
    "- 3: Fatal, at least one death.\n",
    "- 2b: Serious Injury\n",
    "- 2: Injury\n",
    "- 1: Prop damage\n",
    "- 0: Unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Presentation and Discusion__\n",
    "\n",
    "The objective of this data science project is to identify those factors that are most decisive in predicting the severity of a car accident. The information available refers to a dataset of the Seattle area, where it is even possible to identify them by Zone on a map since the latitude and longitude of the location are available. However, it is not possible to map through a geojson because the same dataset does not provide identifiers with the districts.\n",
    "\n",
    "There is information on the number of people involved, the characteristics of the crash, for example, whether it is frontal or lateral, it also refers to whether a parked car or even a pedestrian has participated.\n",
    "Information on weather, and road conditions is also available in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Import Modules__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modules to be used are the basic ones to implement a classification model. They are found within the Sklearn library, in addition to the now classic numpy and pandas.\n",
    "\n",
    "In view of the fact that on some occasions it is essential to be able to have a lightweight model that can be executed in any environment, the choice is to use the simplest versions of sklearn models. However, it is recommended to use other types of classifiers or algorithms. For example, those who are extremely famous in Kaggle competitions, such as LightGBM or XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import metrics\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Cleaning Data and data engineering__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to space issues, all the preprocessing logic is saved, that is, the working method for identifying the most used variables. However, it can be said that both correlation mapping has been done to identify those much more related. In addition, a categorical modification has been implemented through the pandas get_dummies function. Expanding in this way the dataset and helping the algorithm to have a better approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data and validating NANS in it. \n",
    "# Si el nÃºmero de datos NAN es mayor al 50 % entonces no es una columna valida. \n",
    "variables = [\n",
    "    \"SEVERITYCODE\", \"X\", \"Y\", \"ADDRTYPE\", \"HITPARKEDCAR\", \"COLLISIONTYPE\", \"PERSONCOUNT\", \"PEDCOUNT\",\n",
    "    \"PEDCYLCOUNT\", \"VEHCOUNT\", \"UNDERINFL\", \"WEATHER\", \"ROADCOND\", \"LIGHTCOND\", \"SPEEDING\", \"ST_COLCODE\"\n",
    "]\n",
    "data = pd.read_csv(\"datasets/Data-Collisions.csv\")[variables]\n",
    "\n",
    "data[\"SPEEDING\"] = data[\"SPEEDING\"].apply(lambda x: 1 if x == \"Y\" else 0 )\n",
    "valid_columns = []\n",
    "for i in data.columns: \n",
    "    if data[i].isna().sum()/len(data) >0.4: \n",
    "        pass\n",
    "    else: \n",
    "        valid_columns.append(i)\n",
    "        \n",
    "data = data[valid_columns]\n",
    "data.dropna(inplace = True)\n",
    "\n",
    "#Adding Dummies\n",
    "# Convert Categorical Variables: \n",
    "weather = pd.get_dummies(data.WEATHER)\n",
    "road_con = pd.get_dummies(data.ROADCOND)\n",
    "colicion_d  = pd.get_dummies(data.COLLISIONTYPE)\n",
    "light_d  = pd.get_dummies(data.LIGHTCOND)\n",
    "add_d  = pd.get_dummies(data.ADDRTYPE)\n",
    "\n",
    "\n",
    "data= pd.concat([data, weather], axis = 1)\n",
    "data= pd.concat([data, road_con], axis = 1)\n",
    "data= pd.concat([data, colicion_d], axis = 1)\n",
    "data= pd.concat([data, light_d], axis = 1)\n",
    "data= pd.concat([data, add_d], axis = 1)\n",
    "\n",
    "data[\"HITPARKEDCAR\"] = data[\"HITPARKEDCAR\"].apply(lambda x: 0 if x== \"N\" else 1)\n",
    "\n",
    "variable = []\n",
    "\n",
    "for i in data.UNDERINFL: \n",
    "    if i == \"N\": \n",
    "        variable.append(0)\n",
    "    if i == \"0\": \n",
    "        variable.append(0)\n",
    "    if i == \"1\": \n",
    "        variable.append(1)\n",
    "    if i == \"Y\": \n",
    "        variable.append(1)\n",
    "        \n",
    "data[\"UNDERINFL\"]= variable\n",
    "\n",
    "data.drop(columns = [\"WEATHER\", \"ROADCOND\", \"COLLISIONTYPE\", \"LIGHTCOND\", \"ADDRTYPE\"], inplace =True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Model Training__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification model through trees has been used because in addition to being the easiest to implement, they use an easily understandable and powerful internal logic. The main idea is that the data can be classified through different levels of decomposition of the same, or what could be called \"leaves\". It allows to have a much more punctual follow-up on what is taking place under the hood of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"SEVERITYCODE\"]\n",
    "X = data.loc[:, data.columns != 'SEVERITYCODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Results__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model has an acceptable level of accuracy for a prediction (close to 70%), it is possible to have a stable and easily scalable solution, so the possibility of an increase in the amount of information available for training can substantially improve our model's ability to predict class. Furthermore, it is necessary to remember that the classification basically consisted of identifying the level of severity of a crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6929405376052131\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Conclusion__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science models should have two main focuses: \n",
    "   1. Customer; \n",
    "   2. Infrastructure.\n",
    "\n",
    "This means that the most important process in data science is the identification of the business problem we are trying to solve. This implies a continuous and iterative process of interaction with the client, both internal and external, to ensure that the problem has to be solved in the sense that we are giving it. It is also highly recommended to have constant communication with the owners of the MVP in order to guarantee usability.\n",
    "\n",
    "On the other hand, it is also important to take into account the limitations of the infrastructure where the solution will end up being assembled, that is, it is useless to have the trained model with the best performance if it weighs more than 1gb and it is not possible that it be used at through queries to an API, for example. In this sense, it is extremely important to take infrastructure into account. Personally, I consider that the best option, in the case of scalability and ease of use, is Watson, however, for capacity and power, in addition to availability, AWS takes the crown. Not to mention that it has an extremely useful billing alert system, which could help reduce implementation costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
